为 解决 高速 运算 下 ， 存储器 传输速率 受限于 DDR   SDRAM 带宽 而 无法 同步 成长 的 问题 ， 高带宽 存储器 （ High   Bandwidth   Memory ， HBM ） 应运而生 ， 其 革命性 传输 效率 是 让 核心 运算 元件 充分发挥 效能 的 关键 。 据 TrendForce 集邦 咨询 研究 显示 ， 目前 高端 AI 服务器 GPU 搭载 HBM 已成 主流 ， 预估 2023 年 全球 HBM 需求量 将 年 增近 六成 ， 来到 2.9 亿 GB   ， 2024 年 将 再 成长 三成 。
TrendForce 集邦 咨询 预估 到 2025 年 ， 全球 若 以 等同 ChatGPT 的 超大型 AIGC 产品 5 款 、 Midjourney 的 中型 AIGC 产品 有 25 款 ， 以及   80 款 小型 AIGC 产品 估算 ， 上述 所 需 的 运算 资源 至少 为 145 , 600 ~ 233 , 700 颗 NVIDIA   A100   GPU ， 再 加上 新兴 应用 如 超级计算机 、 8K 影音 串流 、 AR / VR 等 ， 也 将 同步 提高 云端 运算 系统 的 负载 ， 高速 运算 需求 高涨 。
由于 HBM 拥有 比 DDR   SDRAM 更 高 的 带宽 和 较 低 的 耗能 ， 无疑 是 建构 高速 运算 平台 的 最佳 解决方案 ， 从 2014 与 2020 年 分别 发布 的 DDR4   SDRAM 及 DDR5   SDRAM 便 可 究其原因 ， 两者 频宽 仅 相差 两倍 ， 且不论 是 DDR5 或 未来 DDR6 ， 在 追求 更高 传输 效能 的 同时 ， 耗电量 将 同步 攀升 ， 势必 拖累 运算 系统 的 效能 表现 。 若 进一步 以 HBM3 与 DDR5 为例 ， 前者 的 带宽 是 后者 的 15 倍 ， 并且 可以 通过 增加 堆栈 的 颗粒 数量 来 提升 总 带宽 。 此外 ， HBM 可以 取代 一部分 的 GDDR   SDRAM 或 DDR   SDRAM ， 从而 更 有效 地 控制 耗能 。
TrendForce 集邦 咨询 表示 ， 目前 主要 由 搭载 NVIDIA   A100 、 H100 、 AMD   MI300 ， 以及 大型 CSP 业者 如 Google 、 AWS 等 自主 研发 ASIC 的 AI 服务器 成长 需求 较为 强劲 ， 2023 年 AI 服务器 出货量 （ 包含 搭载 GPU 、 FPGA 、 ASIC 等 ） 出货量 预估 近 120 万台 ， 年增率 近 38% ， AI 芯片 出货量 同步 看涨 ， 可望 成长 突破 五成 。
“ 掌 ” 握 科技 鲜闻     （ 微信 搜索 techsina 或 扫描 左侧 二维码 关注 ）
新浪 科技
新浪 科技 为 你 带来 最 新鲜 的 科技 资讯
苹果 汇
苹果 汇为 你 带来 最 新鲜 的 苹果 产品 新闻
新浪 众测
新酷 产品 第一 时间 免费 试玩
新浪 探索
提供 最新 的 科学家 新闻 ， 精彩 的 震撼 图片
新浪 科技 意见反馈 留言板
