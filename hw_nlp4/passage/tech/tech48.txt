为解决高速运算下，存储器传输速率受限于DDR SDRAM带宽而无法同步成长的问题，高带宽存储器（High Bandwidth Memory，HBM）应运而生，其革命性传输效率是让核心运算元件充分发挥效能的关键。据TrendForce集邦咨询研究显示，目前高端AI服务器GPU搭载HBM已成主流，预估2023年全球HBM需求量将年增近六成，来到2.9亿GB ，2024年将再成长三成。
TrendForce集邦咨询预估到2025年，全球若以等同ChatGPT的超大型AIGC产品5款、Midjourney的中型AIGC产品有25款，以及 80款小型AIGC产品估算，上述所需的运算资源至少为145,600~233,700颗NVIDIA A100 GPU，再加上新兴应用如超级计算机、8K影音串流、AR/VR等，也将同步提高云端运算系统的负载，高速运算需求高涨。
由于HBM拥有比DDR SDRAM更高的带宽和较低的耗能，无疑是建构高速运算平台的最佳解决方案，从2014与2020年分别发布的DDR4 SDRAM及DDR5 SDRAM便可究其原因，两者频宽仅相差两倍，且不论是DDR5或未来DDR6，在追求更高传输效能的同时，耗电量将同步攀升，势必拖累运算系统的效能表现。若进一步以HBM3与DDR5为例，前者的带宽是后者的15倍，并且可以通过增加堆栈的颗粒数量来提升总带宽。此外，HBM可以取代一部分的GDDR SDRAM或DDR SDRAM，从而更有效地控制耗能。
TrendForce集邦咨询表示，目前主要由搭载NVIDIA A100、H100、AMD MI300，以及大型CSP业者如Google、AWS等自主研发ASIC的AI服务器成长需求较为强劲，2023年AI服务器出货量（包含搭载GPU、FPGA、ASIC等）出货量预估近120万台，年增率近38%，AI芯片出货量同步看涨，可望成长突破五成。
“掌”握科技鲜闻  （微信搜索techsina或扫描左侧二维码关注）
新浪科技
新浪科技为你带来最新鲜的科技资讯
苹果汇
苹果汇为你带来最新鲜的苹果产品新闻
新浪众测
新酷产品第一时间免费试玩
新浪探索
提供最新的科学家新闻，精彩的震撼图片
新浪科技意见反馈留言板
