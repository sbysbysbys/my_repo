{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "def has_chinese(text):\n",
    "    pattern = re.compile(r'[\\u4e00-\\u9fff]')\n",
    "    match = re.search(pattern, text)\n",
    "    return match is not None\n",
    "\n",
    "\n",
    "def get_urls(url):\n",
    "    header = {\"User-Agent\":\n",
    "                    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                    \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                    \"Chrome/114.0.0.0 Safari/537.36\"}\n",
    "    req = requests.get(url=url, headers=header)\n",
    "    req.encoding = \"utf-8\"\n",
    "    html = req.text\n",
    "    bes = BeautifulSoup(html, \"lxml\")\n",
    "    # print(bes)\n",
    "\n",
    "    urls = []\n",
    "    # for text in bes.find_all(\"div\",class_=\"ty-card-r\"):\n",
    "    # for text in bes.find_all(\"h3\",class_=\"ty-card-tt\"):\n",
    "        # texta = text.find(\"a\",target=\"_blank\")\n",
    "        # url1 = texta.get(\"href\")\n",
    "    for text in bes.find_all(\"a\",target=\"_blank\"):\n",
    "        if not text.get(\"href\"):\n",
    "            continue\n",
    "        url1 = text.get(\"href\")\n",
    "        if url1[-5:] == \"shtml\" and url1[:5] == \"https\" and has_chinese(text.get_text()):\n",
    "            print(text)\n",
    "            print(url1)\n",
    "            urls.append(url1)\n",
    "    return urls\n",
    "        \n",
    "\n",
    "urlsport = \"https://sports.sina.com.cn/\"\n",
    "urlent = \"https://ent.sina.com.cn/\"\n",
    "urledu = \"https://edu.sina.com.cn/\"\n",
    "sports_urls = get_urls(urlsport)\n",
    "ent_urls = get_urls(urlent)\n",
    "edu_urls = get_urls(urledu)\n",
    "fashion_urls = get_urls(\"https://fashion.sina.com.cn/\")\n",
    "tech_urls = get_urls(\"https://tech.sina.com.cn/\")\n",
    "print(len(sports_urls))\n",
    "print(len(ent_urls))\n",
    "print(len(edu_urls))\n",
    "print(len(fashion_urls))\n",
    "print(len(tech_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#写入文件\n",
    "header = {\"User-Agent\":\n",
    "                        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                        \"Chrome/114.0.0.0 Safari/537.36\"}\n",
    "for i in range(60):\n",
    "    print(i,\":\")\n",
    "    with open(\"passage\\\\edu\\\\edu%02d.txt\"%i, \"w\", encoding=\"utf-8\") as f:\n",
    "        url = edu_urls[i]\n",
    "        print(url)\n",
    "        req = requests.get(url=url, headers=header)\n",
    "        req.encoding = \"utf-8\"\n",
    "        html = req.text\n",
    "        bes = BeautifulSoup(html, \"lxml\")\n",
    "        # print(bes)\n",
    "        edu_text = bes.find_all(\"p\")\n",
    "        # print(edu_text)\n",
    "        for text in edu_text[:-3]:\n",
    "            for line in text.get_text().split(\"\\n\"):\n",
    "                if has_chinese(line):\n",
    "                    # print(line)\n",
    "                    f.write(line)\n",
    "                    f.write(\"\\n\")\n",
    "        f.close()\n",
    "\n",
    "for i in range(60):\n",
    "    print(i,\":\")\n",
    "    with open(\"passage\\\\ent\\\\ent%02d.txt\"%i, \"w\", encoding=\"utf-8\") as f:\n",
    "        url = ent_urls[i]\n",
    "        print(url)\n",
    "        req = requests.get(url=url, headers=header)\n",
    "        req.encoding = \"utf-8\"\n",
    "        html = req.text\n",
    "        bes = BeautifulSoup(html, \"lxml\")\n",
    "        # print(bes)\n",
    "        ent_text = bes.find_all(\"p\")\n",
    "        # print(ent_text)\n",
    "        for text in ent_text[:-3]:\n",
    "            for line in text.get_text().split(\"\\n\"):\n",
    "                if has_chinese(line):\n",
    "                    # print(line)\n",
    "                    f.write(line)\n",
    "                    f.write(\"\\n\")\n",
    "        f.close()\n",
    "\n",
    "for i in range(60):\n",
    "    print(i,\":\")\n",
    "    with open(\"passage\\\\fashion\\\\fashion%02d.txt\"%i, \"w\", encoding=\"utf-8\") as f:\n",
    "        url = fashion_urls[i]\n",
    "        print(url)\n",
    "        req = requests.get(url=url, headers=header)\n",
    "        req.encoding = \"utf-8\"\n",
    "        html = req.text\n",
    "        bes = BeautifulSoup(html, \"lxml\")\n",
    "        # print(bes)\n",
    "        fashion_text = bes.find_all(\"p\")\n",
    "        # print(fashion_text)\n",
    "        for text in fashion_text[:-3]:\n",
    "            for line in text.get_text().split(\"\\n\"):\n",
    "                if has_chinese(line):\n",
    "                    # print(line)\n",
    "                    f.write(line)\n",
    "                    f.write(\"\\n\")\n",
    "        f.close()\n",
    "\n",
    "for i in range(60):\n",
    "    print(i,\":\")\n",
    "    with open(\"passage\\\\tech\\\\tech%02d.txt\"%i, \"w\", encoding=\"utf-8\") as f:\n",
    "        url = tech_urls[i]\n",
    "        print(url)\n",
    "        req = requests.get(url=url, headers=header)\n",
    "        req.encoding = \"utf-8\"\n",
    "        html = req.text\n",
    "        bes = BeautifulSoup(html, \"lxml\")\n",
    "        # print(bes)\n",
    "        tech_text = bes.find_all(\"p\")\n",
    "        # print(tech_text)\n",
    "        for text in tech_text[:-3]:\n",
    "            for line in text.get_text().split(\"\\n\"):\n",
    "                if has_chinese(line):\n",
    "                    # print(line)\n",
    "                    f.write(line)\n",
    "                    f.write(\"\\n\")\n",
    "        f.close()\n",
    "                        \n",
    "for i in range(25):\n",
    "    print(i,\":\")\n",
    "    with open(\"passage\\\\sports\\\\sports%02d.txt\"%i, \"w\", encoding=\"utf-8\") as f:\n",
    "        url = sports_urls[i]\n",
    "        print(url)\n",
    "        req = requests.get(url=url, headers=header)\n",
    "        req.encoding = \"utf-8\"\n",
    "        html = req.text\n",
    "        bes = BeautifulSoup(html, \"lxml\")\n",
    "        # print(bes)\n",
    "        sports_text = bes.find_all(\"p\")\n",
    "        # print(sports_text)\n",
    "        for text in sports_text[:-3]:\n",
    "            for line in text.get_text().split(\"\\n\"):\n",
    "                if has_chinese(line):\n",
    "                    # print(line)\n",
    "                    f.write(line)\n",
    "                    f.write(\"\\n\")\n",
    "        f.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
