{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"embedding\" not in name:\n",
    "            torch.nn.init.uniform_(\n",
    "                param, a=-0.1, b=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close()>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选出前1000大的词\n",
    "def find_max1000():\n",
    "    f = open('ChineseCorpus199801.txt', encoding = 'gbk')\n",
    "    f_list = f.read().strip('\\n').split()\n",
    "    # for i in range(0,100):\n",
    "    #     print(f_list[i])\n",
    "    count = {}\n",
    "    for word in f_list:\n",
    "        if word in count:\n",
    "            count[word] = count[word]+1\n",
    "        else:\n",
    "            count[word] = 1\n",
    "    sort=sorted(count.items(), key=lambda item:item[1],reverse=True)\n",
    "    max_words = []\n",
    "    for i in range(0,999):\n",
    "        max_words.append(sort[i][0])\n",
    "    # print(max_words[50])\n",
    "    f.close\n",
    "    return max_words\n",
    "\n",
    "\n",
    "max_words=find_max1000()\n",
    "f1 = open('ChineseCorpus199801.txt', encoding = 'gbk')\n",
    "\n",
    "\n",
    "processed_text = []\n",
    "\n",
    "\n",
    "with open('1998.txt','w') as f2:\n",
    "    for line in f1:\n",
    "        words = line.split()\n",
    "        new_line = ' '.join(words[1:])\n",
    "        # print(new_line)\n",
    "        processed_text.append('<START>')\n",
    "        for word in new_line.split():\n",
    "            if word not in max_words:\n",
    "                processed_text.append(\"<UNK>\")\n",
    "            else:\n",
    "                processed_text.append(word)\n",
    "        processed_text.append('<END>')\n",
    "        f2.write(\" \".join(processed_text))\n",
    "        f2.write(\"\\n\")\n",
    "        processed_text = []\n",
    "f2.close   \n",
    "f1.close\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def split_data(text, train_ratio=0.9, random_seed=42):\n",
    "    \"\"\"将语料库分成训练集和测试集\"\"\"\n",
    "    random.seed(random_seed)\n",
    "    data = text.split('\\n')\n",
    "    random.shuffle(data)\n",
    "    split_idx = int(len(data) * train_ratio)\n",
    "    train_data = data[:split_idx]\n",
    "    test_data = data[split_idx:]\n",
    "    return train_data, test_data\n",
    "\n",
    "with open('1998.txt', 'r', encoding='gbk') as f:\n",
    "    text = f.read()\n",
    "    train_data, test_data = split_data(text)\n",
    "\n",
    "with open('train.txt', 'w', encoding='gbk') as f:\n",
    "    f.write('\\n'.join(train_data))\n",
    "    \n",
    "with open('test.txt', 'w', encoding='gbk') as f:\n",
    "    f.write('\\n'.join(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f503ef3360841e598803cee1d2b8c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch0:   0%|          | 0/15823 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 57332.31 \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e2e71f21f3462a8e72aaf68847995f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch1:   0%|          | 0/15823 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 53765.83 \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57380a5f485e4e9ca5b3669bd70b757e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch2:   0%|          | 0/15823 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 52878.38 \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ff0a8a826848d2b03e7867fdad4508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch3:   0%|          | 0/15823 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 52376.03 \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dafce6b94c0402d9cce63a7438142e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch4:   0%|          | 0/15823 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 52033.37 \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86fe1acfde344207942bf317b8ff6583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch5:   0%|          | 0/15823 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 51783.10 \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78337756a4664b20aa9e29ac6d692344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch6:   0%|          | 0/15823 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 51581.95 \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc7dccd4b67d4e67b39a25b029414fa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch7:   0%|          | 0/15823 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 51416.88 \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c194e0654ac54e1e9421978719228360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch8:   0%|          | 0/15823 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 51278.63 \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a42dd66967de4f4086b83eb42b74b2f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch9:   0%|          | 0/15823 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 51154.98 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 定义自定义数据集\n",
    "# f=open('result.txt', 'w', encoding='utf-8')\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, file_path,context_size=2):\n",
    "        self.vocab = find_max1000()\n",
    "        self.vocab.append('<UNK>')\n",
    "        self.vocab.append('<START>')\n",
    "        self.vocab.append('<END>')\n",
    "        self.word_to_idx = {word: i for i, word in enumerate(self.vocab)}\n",
    "        self.data = []\n",
    "        f = open(file_path, 'r', encoding='gbk')\n",
    "        for sentence in f:\n",
    "            split_sentence = sentence.split()\n",
    "            if len(split_sentence)<context_size:\n",
    "                continue\n",
    "            # print(split_sentence)\n",
    "            for i in range(context_size,len(split_sentence)):\n",
    "                context = []\n",
    "                for j in range(0,context_size):\n",
    "                    # print(split_sentence[i-context_size+j],' ')\n",
    "                    # print(self.word_to_idx[split_sentence[i-context_size+j]],'\\n')\n",
    "                    context.append(self.word_to_idx[split_sentence[i-context_size+j]])\n",
    "                target = self.word_to_idx[split_sentence[i]]\n",
    "                self.data.append((context,target))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)   \n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i]\n",
    "    \n",
    "    def collate_fn(self,examples):\n",
    "        # 从独立样本集合中构建批次的输入输出，并转换为PyTorch张量类型\n",
    "        inputs = torch.tensor([ex[0] for ex in examples], dtype=torch.long)\n",
    "        targets = torch.tensor([ex[1] for ex in examples], dtype=torch.long)\n",
    "        return (inputs, targets)\n",
    "\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size, hidden_dim):\n",
    "        super(FNN, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, hidden_dim)\n",
    "        # 线性变换 隐含层-->输出层\n",
    "        self.linear2 = nn.Linear(hidden_dim, vocab_size)\n",
    "        # 使用relu激活函数\n",
    "        self.activate = F.tanh\n",
    "        init_weights(self)\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # 将输入词序列隐射为词向量，并通过view函数对映射后的词向量序列组成的三维张量进行重构，以完成词向量的拼接\n",
    "        embeds = self.embeddings(inputs).view((inputs.shape[0], -1))\n",
    "        # f.write(str(embeds))\n",
    "        # f.write('\\n\\n')\n",
    "        hidden = self.activate(self.linear1(embeds))\n",
    "        # f.write(str(hidden))\n",
    "        # f.write('\\n\\n')\n",
    "        output = self.linear2(hidden)\n",
    "        # f.write(str(output))\n",
    "        # f.write('\\n\\n')\n",
    "        # 根据输出层（logits）计算概率分布并取对数，以便于计算对数似然，这里采用的是Pytorch库的log_softmax实现\n",
    "        log_probs = F.log_softmax(output, dim=1)\n",
    "        # f.write(str(log_probs))\n",
    "        # f.write('\\n\\n')\n",
    "        return log_probs\n",
    "    \n",
    "# 定义超参数\n",
    "vocab_size = 1002\n",
    "learning_rate = 0.001\n",
    "embedding_dim = 64\n",
    "context_size = 2\n",
    "hidden_dim = 128\n",
    "batch_size = 64 ##??\n",
    "num_epoch = 10\n",
    "\n",
    "# 加载数据\n",
    "trainset = TextDataset('train.txt')\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, collate_fn=trainset.collate_fn, shuffle=True)\n",
    "testset = TextDataset('test.txt')\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, collate_fn=testset.collate_fn, shuffle=True)\n",
    "# print(len(train_loader),\" \",len(test_loader))\n",
    "# 初始化模型、损失函数和优化器\n",
    "nll_loss = nn.NLLLoss()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = FNN(vocab_size, embedding_dim, context_size, hidden_dim)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 训练模型\n",
    "model.train()\n",
    "total_losses = []\n",
    "for epoch in range(num_epoch):\n",
    "    total_loss = 0\n",
    "    # print(epoch)\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch{epoch}\"):\n",
    "        inputs, targets = [x.to(device) for x in batch]\n",
    "        # f.write(str(inputs)+'\\n\\n')\n",
    "        # f.write(str(targets)+'\\n\\n')\n",
    "        optimizer.zero_grad()\n",
    "        log_probs = model(inputs)\n",
    "        # f.write(str(log_probs)+'\\n\\n')\n",
    "        loss = nll_loss(log_probs, targets)\n",
    "        # f.write(str(loss)+'\\n\\n')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Loss: {total_loss:.2f} \\n\")\n",
    "    total_losses.append(total_loss)\n",
    "# f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.52391183 -0.1378908   0.25304118 ... -0.0791032   0.06418374\n",
      "   0.02273363]\n",
      " [-0.08229662  0.23296782 -0.08079723 ...  0.12347671 -0.17295134\n",
      "   0.43345055]\n",
      " [ 0.8075559  -0.3431224  -1.103272   ...  0.80142564  0.5408267\n",
      "   0.60051745]\n",
      " ...\n",
      " [-0.1212629  -0.05681767  0.449339   ... -0.07256702  0.00277241\n",
      "  -0.12572654]\n",
      " [-0.13879114 -1.2718202  -0.89848506 ...  0.08579179 -0.09071076\n",
      "   0.6431973 ]\n",
      " [ 0.6533947  -1.0947105   0.52237535 ...  0.05760595 -1.2052001\n",
      "  -0.15918042]]\n"
     ]
    }
   ],
   "source": [
    "# 保存词向量\n",
    "word_vectors = model.embeddings.weight.detach().numpy()\n",
    "print(word_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec946ff124f4440860bf550940d8b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing Epoch1:   0%|          | 0/1701 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on test data:  3.346043348172214\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 在测试集上验证词向量的性能,以便横向进行比较\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(test_loader, desc=f\"Testing Epoch1\"):\n",
    "        inputs, targets = [x.to(device) for x in batch]\n",
    "        log_probs = model(inputs)\n",
    "        loss = nll_loss(log_probs, targets)\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    print(\"Average loss on test data: \", avg_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最相近的十个词\n",
    "import numpy as np\n",
    "import random\n",
    "# 计算余弦相似度\n",
    "f1=open('similarity_fnn.txt','w',encoding='utf-8')\n",
    "#f1.write(\"1\")\n",
    "def cosine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "random_number = random.sample(range(0, 998),20)\n",
    "for i in range(0,20):\n",
    "    random_vector = word_vectors[random_number[i]]\n",
    "    similarity = []\n",
    "    for j in range(0,998):\n",
    "        v = word_vectors[j]\n",
    "        similarity.append(cosine_similarity(random_vector, v))\n",
    "        max_values = sorted(similarity)[-11:]\n",
    "        max_values.reverse()\n",
    "        max_indices = []\n",
    "        for val in max_values:\n",
    "            idx = similarity.index(val)\n",
    "            max_indices.append(idx)\n",
    "    f1.write(\"和\"+str(max_words[random_number[i]])+\"最接近的十个词是：\\n\")\n",
    "    for j in range(1,11):\n",
    "        f1.write(\"    \"+str(max_words[max_indices[j]])+'\\n')\n",
    "f1.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
