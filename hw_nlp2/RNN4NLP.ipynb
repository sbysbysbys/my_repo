{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"embedding\" not in name:\n",
    "            torch.nn.init.uniform_(\n",
    "                param, a=-0.1, b=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close()>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选出前1000大的词\n",
    "def find_max1000():\n",
    "    f = open('ChineseCorpus199801.txt', encoding = 'gbk')\n",
    "    f_list = f.read().strip('\\n').split()\n",
    "    # for i in range(0,100):\n",
    "    #     print(f_list[i])\n",
    "    count = {}\n",
    "    for word in f_list:\n",
    "        if word in count:\n",
    "            count[word] = count[word]+1\n",
    "        else:\n",
    "            count[word] = 1\n",
    "    sort=sorted(count.items(), key=lambda item:item[1],reverse=True)\n",
    "    max_words = []\n",
    "    for i in range(0,999):\n",
    "        max_words.append(sort[i][0])\n",
    "    # print(max_words[50])\n",
    "    f.close\n",
    "    return max_words\n",
    "\n",
    "\n",
    "max_words=find_max1000()\n",
    "f1 = open('ChineseCorpus199801.txt', encoding = 'gbk')\n",
    "\n",
    "\n",
    "processed_text = []\n",
    "\n",
    "\n",
    "with open('1998.txt','w') as f2:\n",
    "    for line in f1:\n",
    "        words = line.split()\n",
    "        new_line = ' '.join(words[1:])\n",
    "        # print(new_line)\n",
    "        processed_text.append('<START>')\n",
    "        for word in new_line.split():\n",
    "            if word not in max_words:\n",
    "                processed_text.append(\"<UNK>\")\n",
    "            else:\n",
    "                processed_text.append(word)\n",
    "        processed_text.append('<END>')\n",
    "        f2.write(\" \".join(processed_text))\n",
    "        f2.write(\"\\n\")\n",
    "        processed_text = []\n",
    "f2.close   \n",
    "f1.close\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def split_data(text, train_ratio=0.9, random_seed=42):\n",
    "    \"\"\"将语料库分成训练集和测试集\"\"\"\n",
    "    random.seed(random_seed)\n",
    "    data = text.split('\\n')\n",
    "    random.shuffle(data)\n",
    "    split_idx = int(len(data) * train_ratio)\n",
    "    train_data = data[:split_idx]\n",
    "    test_data = data[split_idx:]\n",
    "    return train_data, test_data\n",
    "\n",
    "with open('1998.txt', 'r', encoding='gbk') as f:\n",
    "    text = f.read()\n",
    "    train_data, test_data = split_data(text)\n",
    "\n",
    "with open('train.txt', 'w', encoding='gbk') as f:\n",
    "    f.write('\\n'.join(train_data))\n",
    "    \n",
    "with open('test.txt', 'w', encoding='gbk') as f:\n",
    "    f.write('\\n'.join(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55fd8e662f34e8ba04d449ac39efc21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch0:   0%|          | 0/15823 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 57215.14 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 定义自定义数据集\n",
    "# f=open('result.txt', 'w', encoding='utf-8')\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, file_path,context_size=2):\n",
    "        self.vocab = find_max1000()\n",
    "        self.vocab.append('<UNK>')\n",
    "        self.vocab.append('<START>')\n",
    "        self.vocab.append('<END>')\n",
    "        self.word_to_idx = {word: i for i, word in enumerate(self.vocab)}\n",
    "        self.data = []\n",
    "        f = open(file_path, 'r', encoding='gbk')\n",
    "        for sentence in f:\n",
    "            split_sentence = sentence.split()\n",
    "            if len(split_sentence)<context_size:\n",
    "                continue\n",
    "            # print(split_sentence)\n",
    "            for i in range(context_size,len(split_sentence)):\n",
    "                context = []\n",
    "                for j in range(0,context_size):\n",
    "                    # print(split_sentence[i-context_size+j],' ')\n",
    "                    # print(self.word_to_idx[split_sentence[i-context_size+j]],'\\n')\n",
    "                    context.append(self.word_to_idx[split_sentence[i-context_size+j]])\n",
    "                target = self.word_to_idx[split_sentence[i]]\n",
    "                self.data.append((context,target))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)   \n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i]\n",
    "    \n",
    "    def collate_fn(self,examples):\n",
    "        # 从独立样本集合中构建批次的输入输出，并转换为PyTorch张量类型\n",
    "        inputs = torch.tensor([ex[0] for ex in examples], dtype=torch.long)\n",
    "        targets = torch.tensor([ex[1] for ex in examples], dtype=torch.long)\n",
    "        return (inputs, targets)\n",
    "\n",
    "# class RNN(nn.Module):\n",
    "#     def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "#         super(RNN, self).__init__()\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "#         self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "#         self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "#         init_weights(self)\n",
    "        \n",
    "#     def forward(self, inputs):\n",
    "#         embedded = self.embedding(inputs)\n",
    "#         output, hidden = self.rnn(embedded)\n",
    "#         out = self.fc(output[:, -1, :])\n",
    "#         log_probs = F.log_softmax(out, dim=1)\n",
    "#         return log_probs\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size, hidden_dim):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim * context_size, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.activate = nn.Tanh()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        init_weights(self)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.shape[0]\n",
    "        # 将输入词序列隐射为词向量，并通过view函数对映射后的词向量序列组成的三维张量进行重构，以完成词向量的拼接\n",
    "        embeds = self.embeddings(inputs)\n",
    "        # 将词向量序列进行reshape，以将前context_size个词向量组成的张量表示成一个batch\n",
    "        rnn_input = embeds.view((batch_size, -1, embedding_dim*context_size))\n",
    "        # f.write(str(embeds)+'\\n\\n')\n",
    "        hidden = torch.zeros(1, batch_size, self.hidden_dim)\n",
    "        # 将词向量组成的张量输入到RNN模型中，并将模型的输出进行线性变换得到模型的输出层\n",
    "        rnn_out, hidden = self.rnn(rnn_input, hidden)\n",
    "        output = self.linear(rnn_out[:, -1, :])\n",
    "        # 根据输出层（logits）计算概率分布并取对数，以便于计算对数似然，这里采用的是Pytorch库的log_softmax实现\n",
    "        log_probs = F.log_softmax(output, dim=1)\n",
    "        return log_probs\n",
    "    \n",
    "# 定义超参数\n",
    "vocab_size = 1002\n",
    "learning_rate = 0.001\n",
    "embedding_dim = 64\n",
    "context_size = 2\n",
    "hidden_dim = 128\n",
    "batch_size = 64 ##??\n",
    "num_epoch = 10\n",
    "\n",
    "# 加载数据\n",
    "trainset = TextDataset('train.txt')\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, collate_fn=trainset.collate_fn, shuffle=True)\n",
    "testset = TextDataset('test.txt')\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, collate_fn=testset.collate_fn, shuffle=True)\n",
    "# print(len(train_loader),\" \",len(test_loader))\n",
    "# 初始化模型、损失函数和优化器\n",
    "nll_loss = nn.NLLLoss()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = RNN(vocab_size, embedding_dim, context_size, hidden_dim)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 训练模型\n",
    "model.train()\n",
    "total_losses = []\n",
    "for epoch in range(num_epoch):\n",
    "    total_loss = 0\n",
    "    # print(epoch)\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch{epoch}\"):\n",
    "        inputs, targets = [x.to(device) for x in batch]\n",
    "        # f.write(str(inputs)+'\\n\\n')\n",
    "        # f.write(str(targets)+'\\n\\n')\n",
    "        optimizer.zero_grad()\n",
    "        log_probs = model(inputs)\n",
    "        # f.write(str(log_probs)+'\\n\\n')\n",
    "        loss = nll_loss(log_probs, targets)\n",
    "        # f.write(str(loss)+'\\n\\n')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Loss: {total_loss:.2f} \\n\")\n",
    "    total_losses.append(total_loss)\n",
    "# f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.43323225 -0.95282775 -1.0422843  ... -0.0813977   0.09066681\n",
      "   0.8428811 ]\n",
      " [-0.6021678  -1.3465147   0.8065184  ...  1.4331632  -0.3015417\n",
      "  -0.26102823]\n",
      " [-0.24474302  1.132196   -1.4061037  ... -1.4256881   0.38201934\n",
      "   0.8074708 ]\n",
      " ...\n",
      " [-0.24710114 -0.84516823 -0.5132379  ...  0.28640217  0.12368841\n",
      "  -0.47149006]\n",
      " [-2.0884485   0.60893506  0.87873    ... -0.85921764  0.0739369\n",
      "   0.54568774]\n",
      " [ 0.23070653 -1.5181859   0.42942083 ...  0.21706979  1.26504\n",
      "   1.9049441 ]]\n"
     ]
    }
   ],
   "source": [
    "# 保存词向量\n",
    "word_vectors = model.embeddings.weight.detach().numpy()\n",
    "print(word_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ef8d0bbf7374f0186519438b83fa866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing Epoch1:   0%|          | 0/1701 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on test data:  3.44497489957232\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 在测试集上验证词向量的性能,以便横向进行比较\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(test_loader, desc=f\"Testing Epoch1\"):\n",
    "        inputs, targets = [x.to(device) for x in batch]\n",
    "        log_probs = model(inputs)\n",
    "        loss = nll_loss(log_probs, targets)\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    print(\"Average loss on test data: \", avg_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最相近的十个词\n",
    "import numpy as np\n",
    "import random\n",
    "# 计算余弦相似度\n",
    "f1=open('similarity_rnn.txt','w',encoding='utf-8')\n",
    "def cosine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "random_number = random.sample(range(0, 998),20)\n",
    "for i in range(0,20):\n",
    "    random_vector = word_vectors[random_number[i]]\n",
    "    similarity = []\n",
    "    for j in range(0,998):\n",
    "        v = word_vectors[j]\n",
    "        similarity.append(cosine_similarity(random_vector, v))\n",
    "        max_values = sorted(similarity)[-11:]\n",
    "        max_values.reverse()\n",
    "        max_indices = []\n",
    "        for val in max_values:\n",
    "            idx = similarity.index(val)\n",
    "            max_indices.append(idx)\n",
    "    f1.write(\"和\"+str(max_words[random_number[i]])+\"最接近的十个词是：\\n\")\n",
    "    for j in range(1,11):\n",
    "        f1.write(\"    \"+str(max_words[max_indices[j]])+'\\n')\n",
    "f1.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
